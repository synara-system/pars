# path: core/scanners/cloud_exploit.py

import aiohttp
import asyncio
import re
from typing import Callable, List, Dict
from urllib.parse import urlparse

from core.scanners.base_scanner import BaseScanner

class CloudExploitScanner(BaseScanner):
    """
    Cloud Exploit Scanner (Cloudstorm Engine)
    -----------------------------------------
    Bulut altyapılarında (AWS, GCP, Azure) çalışan uygulamalar için:
    1. Metadata Servislerine (IMDSv1/v2) yönelik SSRF denemeleri.
    2. Hedef domaine ait olabilecek açık S3 Bucket (Depolama) keşfi.
    
    Bu modül, modern bulut güvenliği risklerini hedefler.
    """

    # Bulut Metadata Endpoint'leri (SSRF Hedefleri)
    METADATA_TARGETS = {
        "AWS": "http://169.254.169.254/latest/meta-data/",
        "GCP": "http://metadata.google.internal/computeMetadata/v1/",
        "Azure": "http://169.254.169.254/metadata/instance?api-version=2021-02-01",
        "DigitalOcean": "http://169.254.169.254/metadata/v1/",
        "Oracle": "http://169.254.169.254/opc/v1/instance/"
    }

    # Başarılı yanıt göstergeleri (Signature)
    SUCCESS_INDICATORS = [
        "ami-id", "instance-id", "reservation-id",  # AWS
        "computeMetadata", "google-compute-engine", # GCP
        "compute", "network", "osProfile",          # Azure
        "droplet_id", "public_keys",               # DigitalOcean
        "canonicalRegionName", "ociAdName"         # Oracle
    ]

    # S3 Bucket varyasyonları için patternler
    BUCKET_PATTERNS = [
        "{domain}",
        "{domain}-backup",
        "{domain}-assets",
        "{domain}-static",
        "{domain}-dev",
        "{domain}-staging",
        "{domain}-logs",
        "backup-{domain}",
        "assets-{domain}"
    ]

    def __init__(self, logger, results_callback, request_callback: Callable[[], None]):
        super().__init__(logger, results_callback, request_callback)

    @property
    def name(self):
        return "Cloud Exploit & Metadata Scanner"

    @property
    def category(self):
        return "CLOUD_EXPLOIT"

    async def scan(self, url: str, session: aiohttp.ClientSession, completed_callback: Callable[[], None]):
        """
        Bulut zafiyet tarama mantığını uygular.
        """
        self.log(f"[{self.category}] Cloudstorm motoru başlatılıyor (Metadata & Bucket Analizi)...", "INFO")

        try:
            domain = self._get_domain(url)
            if not domain:
                self.log(f"[{self.category}] Geçersiz domain, tarama durduruldu.", "WARNING")
                completed_callback()
                return

            tasks = []
            
            # 1. Metadata SSRF Testleri (SSRF zafiyeti varsa buluta ulaşabilir miyiz?)
            # Bu testler genellikle parametre fuzzing ile yapılır (RCE_SSRF modülü gibi),
            # ancak burada doğrudan endpoint'e erişim denemesi değil, hedef uygulamanın
            # bu endpoint'lere yönlendirme yapıp yapmadığını simüle eden parametre enjeksiyonları
            # RCE_SSRF modülünde yapılıyor.
            # BURADA: Doğrudan bucket keşfi ve eğer hedef bir proxy ise metadata çekme denemesi yapılır.
            
            # 2. Public S3 Bucket Keşfi
            # Hedefin ismiyle ilişkili olası S3 bucketlarını tarar.
            tasks.append(self._scan_public_buckets(domain, session))

            # 3. Cloud Headers Analizi (Pasif)
            # Yanıtlarda bulut sağlayıcı izi var mı?
            tasks.append(self._check_cloud_headers(url, session))

            await asyncio.gather(*tasks)

        except Exception as e:
            self.log(f"[{self.category}] Kritik Hata: {type(e).__name__} ({e})", "CRITICAL")
            self.add_result(self.category, "CRITICAL", f"Cloud Exploit Hatası: {str(e)}", 0)

        completed_callback()

    def _get_domain(self, url: str) -> str:
        """URL'den ana domaini (uzantısız) ayıklar (örn: example.com -> example)."""
        try:
            parsed = urlparse(url)
            netloc = parsed.netloc.split(':')[0] # Portu temizle
            # Basitçe domain adını al (subdomainleri yok sayarak ana isme odaklan)
            parts = netloc.split('.')
            if len(parts) >= 2:
                # "www.google.com" -> "google"
                # "test.programevi.com" -> "programevi" (basitleştirilmiş)
                return parts[-2]
            return parts[0]
        except:
            return ""

    async def _scan_public_buckets(self, domain_base: str, session: aiohttp.ClientSession):
        """
        Olası S3 bucket isimlerini türetir ve açık olup olmadıklarını kontrol eder.
        """
        self.log(f"[{self.category}] '{domain_base}' için S3 Bucket avı başlatılıyor...", "INFO")
        
        found_buckets = []
        
        # Concurrency kontrolü için semaphore (Bucket taraması hızlı olabilir)
        sem = asyncio.Semaphore(5)

        async def check_bucket(bucket_name):
            target_bucket_url = f"https://{bucket_name}.s3.amazonaws.com"
            async with sem:
                try:
                    await self._throttled_request(session, "HEAD", target_bucket_url) # Sadece HEAD isteği
                    
                    # HEAD isteği için ayrı bir request yapıyoruz, status code önemli
                    async with session.head(target_bucket_url, timeout=aiohttp.ClientTimeout(total=5)) as res:
                        if res.status == 200:
                            # Açık Bucket (Listeleme açık olabilir)
                            # İçeriği kontrol et (XML döner)
                            async with session.get(target_bucket_url, timeout=aiohttp.ClientTimeout(total=5)) as get_res:
                                content = await get_res.text()
                                if "ListBucketResult" in content:
                                    return (bucket_name, "CRITICAL", "Açık S3 Bucket (Listelenebilir)", target_bucket_url)
                                else:
                                    return (bucket_name, "HIGH", "Açık S3 Bucket (Erişilebilir)", target_bucket_url)
                        
                        elif res.status == 403:
                            # Bucket var ama kapalı (Bilgi ifşası: Bucket isminin varlığı doğrulanmış olur)
                            return (bucket_name, "INFO", "S3 Bucket Mevcut (Erişim Engelli)", target_bucket_url)
                            
                except Exception:
                    pass
            return None

        tasks = []
        for pattern in self.BUCKET_PATTERNS:
            bucket_name = pattern.format(domain=domain_base)
            tasks.append(check_bucket(bucket_name))
            
        results = await asyncio.gather(*tasks)
        
        for res in results:
            if res:
                bucket_name, level, msg, link = res
                score = self._calculate_score_deduction(level)
                self.add_result(self.category, level, f"{msg}: {link}", score)
                self.log(f"[{self.category}] {msg}: {link}", level)
                found_buckets.append(bucket_name)

        if not found_buckets:
            self.log(f"[{self.category}] Açık S3 bucket tespit edilemedi.", "INFO")

    async def _check_cloud_headers(self, url: str, session: aiohttp.ClientSession):
        """
        Hedef URL'nin başlıklarını analiz ederek hangi bulut sağlayıcısında olduğunu tespit eder.
        """
        try:
            self.request_callback()
            async with session.get(url, allow_redirects=True, timeout=aiohttp.ClientTimeout(total=10)) as res:
                headers = res.headers
                
                cloud_provider = None
                
                # AWS
                if "x-amz-id-2" in headers or "x-amz-request-id" in headers or "Awselb" in headers.get("Server", ""):
                    cloud_provider = "Amazon Web Services (AWS)"
                
                # GCP
                elif "x-guploader-uploadid" in headers or "x-goog-generation" in headers:
                    cloud_provider = "Google Cloud Platform (GCP)"
                
                # Azure
                elif "x-ms-request-id" in headers or "x-azure-ref" in headers:
                    cloud_provider = "Microsoft Azure"
                
                # Cloudflare (WAF/CDN)
                elif "cf-ray" in headers:
                    cloud_provider = "Cloudflare (CDN/WAF)"

                if cloud_provider:
                    self.add_result(self.category, "INFO", f"Altyapı Tespiti: Hedef sistem {cloud_provider} üzerinde barınıyor olabilir.", 0)
                    self.log(f"[{self.category}] Altyapı İmzası: {cloud_provider}", "INFO")

        except Exception:
            pass